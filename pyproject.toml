[project]
name = "llmserve"
version = "0.1.0"
description = "Production-ready LLM serving with vLLM, disaggregated prefill/decode, continuous batching, tiered KV-cache (GPU → DRAM → NVMe → Object), NIXL/UCX/NCCL transfer engines, speculative decoding, and a fair-share scheduler. Single command brings the whole stack up from a YAML manifest."
authors = [
    {name = "pritaman",email = "nayak264@gmail.com"}
]
license = {text = "MIT License"}
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
  "typer>=0.12",
  "rich>=13.7",
  "pyyaml>=6.0",
  "pydantic>=2.7",
  "fastapi>=0.111",
  "uvicorn[standard]>=0.30",
  "prometheus-client>=0.20",
  # add more as we wire engines (vllm, transformers, etc.)
  "orjson>=3.10",
  "vllm>=0.10",
]

[project.optional-dependencies]
lookahead = [
  "lade>=0.1.4",                # LookaheadDecoding pip package
  "transformers>=4.41",
  "accelerate>=0.30",
  # One of these two (faster if available):
  # vanilla FA:
  "flash-attn==2.3.3",
  # OR specialized LADE FA wheel (manual install per GPU/Torch/Python matrix per README)
]

[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[project.scripts]
llmserve = "llmserve.cli:app"
